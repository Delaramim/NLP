{"cells":[{"metadata":{},"cell_type":"markdown","source":"NLP Text Summarization","attachments":{}},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -U spacy\n\n!python -m spacy download en_core_web_sm","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting spacy\n  Downloading spacy-2.3.5-cp37-cp37m-manylinux2014_x86_64.whl (10.4 MB)\n\u001b[K     |████████████████████████████████| 10.4 MB 4.1 MB/s eta 0:00:01    |█████████████████▋              | 5.7 MB 4.1 MB/s eta 0:00:02\n\u001b[?25hCollecting numpy>=1.15.0\n  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n\u001b[K     |████████████████████████████████| 14.8 MB 43.9 MB/s eta 0:00:01\n\u001b[?25hCollecting plac<1.2.0,>=0.9.6\n  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\nRequirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy) (2.24.0)\nCollecting cymem<2.1.0,>=2.0.2\n  Downloading cymem-2.0.5-cp37-cp37m-manylinux2014_x86_64.whl (35 kB)\nCollecting preshed<3.1.0,>=3.0.2\n  Downloading preshed-3.0.5-cp37-cp37m-manylinux2014_x86_64.whl (126 kB)\n\u001b[K     |████████████████████████████████| 126 kB 32.9 MB/s eta 0:00:01\n\u001b[?25hCollecting blis<0.8.0,>=0.4.0\n  Downloading blis-0.7.4-cp37-cp37m-manylinux2014_x86_64.whl (9.8 MB)\n\u001b[K     |████████████████████████████████| 9.8 MB 42.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy) (49.2.0.post20200712)\nCollecting thinc<7.5.0,>=7.4.1\n  Downloading thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0 MB)\n\u001b[K     |████████████████████████████████| 1.0 MB 42.2 MB/s eta 0:00:01\n\u001b[?25hCollecting srsly<1.1.0,>=1.0.2\n  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n\u001b[K     |████████████████████████████████| 184 kB 27.4 MB/s eta 0:00:01\n\u001b[?25hCollecting wasabi<1.1.0,>=0.4.0\n  Downloading wasabi-0.8.0-py3-none-any.whl (23 kB)\nCollecting murmurhash<1.1.0,>=0.28.0\n  Downloading murmurhash-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (20 kB)\nCollecting tqdm<5.0.0,>=4.38.0\n  Downloading tqdm-4.55.2-py2.py3-none-any.whl (68 kB)\n\u001b[K     |████████████████████████████████| 68 kB 8.8 MB/s  eta 0:00:01\n\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\nRequirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.10)\nRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\nRequirement already satisfied, skipping upgrade: idna<3,>=2.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\nRequirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\nRequirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /srv/conda/envs/notebook/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\nRequirement already satisfied, skipping upgrade: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\nInstalling collected packages: numpy, plac, cymem, murmurhash, preshed, blis, srsly, tqdm, wasabi, catalogue, thinc, spacy\nSuccessfully installed blis-0.7.4 catalogue-1.0.0 cymem-2.0.5 murmurhash-1.0.5 numpy-1.19.5 plac-1.1.3 preshed-3.0.5 spacy-2.3.5 srsly-1.0.5 thinc-7.4.5 tqdm-4.55.2 wasabi-0.8.0\nCollecting en_core_web_sm==2.3.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n\u001b[K     |████████████████████████████████| 12.0 MB 1.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.5)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\nRequirement already satisfied: numpy>=1.15.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.5)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\nRequirement already satisfied: setuptools in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.0.post20200712)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\nRequirement already satisfied: thinc<7.5.0,>=7.4.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.55.2)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\nRequirement already satisfied: srsly<1.1.0,>=1.0.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\nRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /srv/conda/envs/notebook/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.7.0)\nRequirement already satisfied: chardet<4,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.10)\nRequirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\nRequirement already satisfied: idna<3,>=2.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\nRequirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.1.0)\nBuilding wheels for collected packages: en-core-web-sm\n  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047105 sha256=7d921de14f3007f36569fe450d0af0c797c3a9dde794800b277186a1467b4253\n  Stored in directory: /tmp/pip-ephem-wheel-cache-eh4fpdkb/wheels/b7/0d/f0/7ecae8427c515065d75410989e15e5785dd3975fe06e795cd9\nSuccessfully built en-core-web-sm\nInstalling collected packages: en-core-web-sm\nSuccessfully installed en-core-web-sm-2.3.1\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the model via spacy.load('en_core_web_sm')\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom string import punctuation","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopWords = list(STOP_WORDS)","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"introductionText = \"\"\"\nThe capacity of language is what differentiates humans from other species. Humans learned how to speak around 100000 years ago, and 3000 years after humans learned how to write. Furthermore, this ability to communicate and share information is what makes humans successful. However, human language is one of the most complex and diverse parts of our species; there are 6500 languages spoken in the world. One of the means of communication that we use is communication online. According to industry estimates, only 20% of our data acquired through our messages and online activities has a structured form. The rest of our data is in an unstructured textual form. The Web consists of over a trillion pages of information, and as mentioned, is mostly in natural language. For knowledge acquisition to be possible, an agent needs to partially be able to interpret, the ambiguous natural language used. To be able to do so, it is important to understand the techniques of text analysis and natural language processing. Furthermore, text analysis/ text mining is the process of deriving significant information from natural language text; whereas, natural language processing is part of computer science and artificial intelligence that deals with human languages. This paper will cover natural language processing and some algorithms used such as Tokenization, Stemming, and Lemmatizing.\"\"\"","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm')","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = nlp(introductionText)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens = [token.text for token in doc]\nprint(tokens)","execution_count":20,"outputs":[{"output_type":"stream","text":"['\\n', 'The', 'capacity', 'of', 'language', 'is', 'what', 'differentiates', 'humans', 'from', 'other', 'species', '.', 'Humans', 'learned', 'how', 'to', 'speak', 'around', '100000', 'years', 'ago', ',', 'and', '3000', 'years', 'after', 'humans', 'learned', 'how', 'to', 'write', '.', 'Furthermore', ',', 'this', 'ability', 'to', 'communicate', 'and', 'share', 'information', 'is', 'what', 'makes', 'humans', 'successful', '.', 'However', ',', 'human', 'language', 'is', 'one', 'of', 'the', 'most', 'complex', 'and', 'diverse', 'parts', 'of', 'our', 'species', ';', 'there', 'are', '6500', 'languages', 'spoken', 'in', 'the', 'world', '.', 'One', 'of', 'the', 'means', 'of', 'communication', 'that', 'we', 'use', 'is', 'communication', 'online', '.', 'According', 'to', 'industry', 'estimates', ',', 'only', '20', '%', 'of', 'our', 'data', 'acquired', 'through', 'our', 'messages', 'and', 'online', 'activities', 'has', 'a', 'structured', 'form', '.', 'The', 'rest', 'of', 'our', 'data', 'is', 'in', 'an', 'unstructured', 'textual', 'form', '.', 'The', 'Web', 'consists', 'of', 'over', 'a', 'trillion', 'pages', 'of', 'information', ',', 'and', 'as', 'mentioned', ',', 'is', 'mostly', 'in', 'natural', 'language', '.', 'For', 'knowledge', 'acquisition', 'to', 'be', 'possible', ',', 'an', 'agent', 'needs', 'to', 'partially', 'be', 'able', 'to', 'interpret', ',', 'the', 'ambiguous', 'natural', 'language', 'used', '.', 'To', 'be', 'able', 'to', 'do', 'so', ',', 'it', 'is', 'important', 'to', 'understand', 'the', 'techniques', 'of', 'text', 'analysis', 'and', 'natural', 'language', 'processing', '.', 'Furthermore', ',', 'text', 'analysis/', 'text', 'mining', 'is', 'the', 'process', 'of', 'deriving', 'significant', 'information', 'from', 'natural', 'language', 'text', ';', 'whereas', ',', 'natural', 'language', 'processing', 'is', 'part', 'of', 'computer', 'science', 'and', 'artificial', 'intelligence', 'that', 'deals', 'with', 'human', 'languages', '.', 'This', 'paper', 'will', 'cover', 'natural', 'language', 'processing', 'and', 'some', 'algorithms', 'used', 'such', 'as', 'Tokenization', ',', 'Stemming', ',', 'and', 'Lemmatizing', '.']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"punctuation = punctuation + '\\n'\npunctuation","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordFrequencies = {}\nfor word in doc:\n    if word.text.lower() not in stopWords:\n        if word.text.lower() not in punctuation:\n            if word.text not in wordFrequencies.keys():\n                wordFrequencies[word.text] = 1\n            else:\n                wordFrequencies[word.text] += 1","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(wordFrequencies)","execution_count":23,"outputs":[{"output_type":"stream","text":"{'capacity': 1, 'language': 8, 'differentiates': 1, 'humans': 3, 'species': 2, 'Humans': 1, 'learned': 2, 'speak': 1, '100000': 1, 'years': 2, 'ago': 1, '3000': 1, 'write': 1, 'Furthermore': 2, 'ability': 1, 'communicate': 1, 'share': 1, 'information': 3, 'makes': 1, 'successful': 1, 'human': 2, 'complex': 1, 'diverse': 1, 'parts': 1, '6500': 1, 'languages': 2, 'spoken': 1, 'world': 1, 'means': 1, 'communication': 2, 'use': 1, 'online': 2, 'According': 1, 'industry': 1, 'estimates': 1, '20': 1, 'data': 2, 'acquired': 1, 'messages': 1, 'activities': 1, 'structured': 1, 'form': 2, 'rest': 1, 'unstructured': 1, 'textual': 1, 'Web': 1, 'consists': 1, 'trillion': 1, 'pages': 1, 'mentioned': 1, 'natural': 6, 'knowledge': 1, 'acquisition': 1, 'possible': 1, 'agent': 1, 'needs': 1, 'partially': 1, 'able': 2, 'interpret': 1, 'ambiguous': 1, 'important': 1, 'understand': 1, 'techniques': 1, 'text': 4, 'analysis': 1, 'processing': 3, 'analysis/': 1, 'mining': 1, 'process': 1, 'deriving': 1, 'significant': 1, 'computer': 1, 'science': 1, 'artificial': 1, 'intelligence': 1, 'deals': 1, 'paper': 1, 'cover': 1, 'algorithms': 1, 'Tokenization': 1, 'Stemming': 1, 'Lemmatizing': 1}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxFrequency = max(wordFrequencies.values())\nmaxFrequency","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"8"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for word in wordFrequencies.keys():\n    wordFrequencies[word] = wordFrequencies[word]/maxFrequency\n\nprint(wordFrequencies)","execution_count":25,"outputs":[{"output_type":"stream","text":"{'capacity': 0.125, 'language': 1.0, 'differentiates': 0.125, 'humans': 0.375, 'species': 0.25, 'Humans': 0.125, 'learned': 0.25, 'speak': 0.125, '100000': 0.125, 'years': 0.25, 'ago': 0.125, '3000': 0.125, 'write': 0.125, 'Furthermore': 0.25, 'ability': 0.125, 'communicate': 0.125, 'share': 0.125, 'information': 0.375, 'makes': 0.125, 'successful': 0.125, 'human': 0.25, 'complex': 0.125, 'diverse': 0.125, 'parts': 0.125, '6500': 0.125, 'languages': 0.25, 'spoken': 0.125, 'world': 0.125, 'means': 0.125, 'communication': 0.25, 'use': 0.125, 'online': 0.25, 'According': 0.125, 'industry': 0.125, 'estimates': 0.125, '20': 0.125, 'data': 0.25, 'acquired': 0.125, 'messages': 0.125, 'activities': 0.125, 'structured': 0.125, 'form': 0.25, 'rest': 0.125, 'unstructured': 0.125, 'textual': 0.125, 'Web': 0.125, 'consists': 0.125, 'trillion': 0.125, 'pages': 0.125, 'mentioned': 0.125, 'natural': 0.75, 'knowledge': 0.125, 'acquisition': 0.125, 'possible': 0.125, 'agent': 0.125, 'needs': 0.125, 'partially': 0.125, 'able': 0.25, 'interpret': 0.125, 'ambiguous': 0.125, 'important': 0.125, 'understand': 0.125, 'techniques': 0.125, 'text': 0.5, 'analysis': 0.125, 'processing': 0.375, 'analysis/': 0.125, 'mining': 0.125, 'process': 0.125, 'deriving': 0.125, 'significant': 0.125, 'computer': 0.125, 'science': 0.125, 'artificial': 0.125, 'intelligence': 0.125, 'deals': 0.125, 'paper': 0.125, 'cover': 0.125, 'algorithms': 0.125, 'Tokenization': 0.125, 'Stemming': 0.125, 'Lemmatizing': 0.125}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentenceTokens = [sentence for sentence in doc.sents]\nprint(sentenceTokens)","execution_count":26,"outputs":[{"output_type":"stream","text":"[\nThe capacity of language is what differentiates humans from other species., Humans learned how to speak around 100000 years ago, and 3000 years after humans learned how to write., Furthermore, this ability to communicate and share information is what makes humans successful., However, human language is one of the most complex and diverse parts of our species; there are 6500 languages spoken in the world., One of the means of communication that we use is communication online., According to industry estimates, only 20% of our data acquired through our messages and online activities has a structured form., The rest of our data is in an unstructured textual form., The Web consists of over a trillion pages of information, and as mentioned, is mostly in natural language., For knowledge acquisition to be possible, an agent needs to partially be able to interpret, the ambiguous natural language used., To be able to do so, it is important to understand the techniques of text analysis and natural language processing., Furthermore, text analysis/ text mining is the process of deriving significant information from natural language text; whereas, natural language processing is part of computer science and artificial intelligence that deals with human languages., This paper will cover natural language processing and some algorithms used such as Tokenization, Stemming, and Lemmatizing.]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentenceScore={}\nfor sentence in sentenceTokens:\n    for word in sentence:\n        if word.text.lower() in wordFrequencies.keys():\n            if sentence not in sentenceScore.keys():\n                sentenceScore[sentence] = wordFrequencies[word.text.lower()]\n            else:\n                sentenceScore[sentence] += wordFrequencies[word.text.lower()]\n                \nsentenceScore","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"{\n The capacity of language is what differentiates humans from other species.: 1.875,\n Humans learned how to speak around 100000 years ago, and 3000 years after humans learned how to write.: 2.375,\n Furthermore, this ability to communicate and share information is what makes humans successful.: 1.375,\n However, human language is one of the most complex and diverse parts of our species; there are 6500 languages spoken in the world.: 2.5,\n One of the means of communication that we use is communication online.: 1.0,\n According to industry estimates, only 20% of our data acquired through our messages and online activities has a structured form.: 1.625,\n The rest of our data is in an unstructured textual form.: 0.875,\n The Web consists of over a trillion pages of information, and as mentioned, is mostly in natural language.: 2.625,\n For knowledge acquisition to be possible, an agent needs to partially be able to interpret, the ambiguous natural language used.: 3.0,\n To be able to do so, it is important to understand the techniques of text analysis and natural language processing.: 3.375,\n Furthermore, text analysis/ text mining is the process of deriving significant information from natural language text; whereas, natural language processing is part of computer science and artificial intelligence that deals with human languages.: 7.5,\n This paper will cover natural language processing and some algorithms used such as Tokenization, Stemming, and Lemmatizing.: 2.5}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from heapq import nlargest","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentenceLength = int(len(sentenceTokens)*0.5)\nsentenceLength","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"6"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary = nlargest(sentenceLength, sentenceScore, key = sentenceScore.get)\nsummary\n","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"[Furthermore, text analysis/ text mining is the process of deriving significant information from natural language text; whereas, natural language processing is part of computer science and artificial intelligence that deals with human languages.,\n To be able to do so, it is important to understand the techniques of text analysis and natural language processing.,\n For knowledge acquisition to be possible, an agent needs to partially be able to interpret, the ambiguous natural language used.,\n The Web consists of over a trillion pages of information, and as mentioned, is mostly in natural language.,\n However, human language is one of the most complex and diverse parts of our species; there are 6500 languages spoken in the world.,\n This paper will cover natural language processing and some algorithms used such as Tokenization, Stemming, and Lemmatizing.]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_summary = [word.text for word in summary]\nsummary = ' '.join(final_summary)","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"'Furthermore, text analysis/ text mining is the process of deriving significant information from natural language text; whereas, natural language processing is part of computer science and artificial intelligence that deals with human languages. To be able to do so, it is important to understand the techniques of text analysis and natural language processing. For knowledge acquisition to be possible, an agent needs to partially be able to interpret, the ambiguous natural language used. The Web consists of over a trillion pages of information, and as mentioned, is mostly in natural language. However, human language is one of the most complex and diverse parts of our species; there are 6500 languages spoken in the world. This paper will cover natural language processing and some algorithms used such as Tokenization, Stemming, and Lemmatizing.'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}